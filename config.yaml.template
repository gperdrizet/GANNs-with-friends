# Configuration for Distributed GAN Training

# ==================== Database (all roles) ====================
database:
  host: YOUR_DATABASE_HOST
  port: 54321
  database: distributed_gan
  user: YOUR_DATABASE_USER
  password: YOUR_DATABASE_PASSWORD

# ==================== Training ====================
training:
  # Model architecture (both coordinator and workers)
  latent_dim: 100             # Dimension of generator input noise
  image_size: 64              # Image size (64x64)
  
  # Coordinator only
  images_per_work_unit: 320   # Images per work unit assignment
  num_workunits_per_update: 3 # Work units to collect before model update
  learning_rate: 0.0002       # Adam optimizer learning rate
  beta1: 0.5                  # Adam beta1
  beta2: 0.999                # Adam beta2

# ==================== Worker settings ====================
worker:
  name:                   # Optional: Your name or identifier (shown in dashboard)
  batch_size: 32          # Images per batch (adjust based on your GPU memory)
  poll_interval: 5        # seconds between work unit polls
  heartbeat_interval: 30  # seconds between heartbeat updates
  work_unit_timeout: 300  # seconds before work unit times out

# ==================== Data (all roles) ====================
data:
  dataset_path: data/celeba_torchvision/celeba/img_align_celeba  # Downloaded automatically if not present
  num_workers_dataloader: 4  # Number of dataloader workers

# ==================== Hugging Face (coordinator only) ====================
huggingface:
  # Default repo allows downloading pre-trained models and dataset (no token needed)
  # To run your own training as coordinator:
  #   1. Create your own repo at huggingface.co/new
  #   2. Set repo_id to: YOUR_USERNAME/YOUR_REPO_NAME
  #   3. Add your token from huggingface.co/settings/tokens (needs write access)
  #   4. Set enabled: true

  enabled: false
  repo_id: gperdrizet/GANNs-with-friends  # Default: project repo (read-only for downloads)
  token: ''  # Only needed if pushing to your own repo
  push_interval: 5  # Push model every N iterations
